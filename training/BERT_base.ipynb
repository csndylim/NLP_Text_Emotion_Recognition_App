{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18578,"status":"ok","timestamp":1635401147231,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"v1L-D4onRFZr","outputId":"968f2f87-fdb3-4a81-ee8c-c9c2d1b1ccbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OW9pCCPaQJa6"},"source":["### Download dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9074,"status":"ok","timestamp":1635401160366,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"wjcAMvXqtRSn","outputId":"0e8bd0e8-859c-4d7a-d67f-dca9e7531adb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 8.1 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 51.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 52.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 56.6 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"]}],"source":["!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7732,"status":"ok","timestamp":1635401168095,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"ENUrJ4ywL9ti","outputId":"c7630375-3c53-46e0-d108-63d494cbe217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.46)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n","Collecting boto3\n","  Downloading boto3-1.19.5-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 56.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.9.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n","Collecting botocore<1.23.0,>=1.22.5\n","  Downloading botocore-1.22.5-py3-none-any.whl (8.1 MB)\n","\u001b[K     |████████████████████████████████| 8.1 MB 53.7 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.5->boto3->pytorch-transformers) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.5->boto3->pytorch-transformers) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.5.30)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 59.9 MB/s \n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.19.5 botocore-1.22.5 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sentencepiece-0.1.96 urllib3-1.25.11\n"]}],"source":["!pip install pytorch-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14614,"status":"ok","timestamp":1635401182707,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"mITSXunyLVR1","outputId":"97040fdb-b819-45c1-f1f9-450eb1ce7c09"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.11.3\n"]}],"source":["from transformers import __version__\n","print(__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3220,"status":"ok","timestamp":1635401185918,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"tNnZqTbH1Owv","outputId":"80468233-3e0f-43a5-b362-4c752cfbc2e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8999,"status":"ok","timestamp":1635401194913,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"6MiGYWogQK7L","outputId":"7e25b677-cd95-48c0-993a-364f041bd8e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wandb\n","  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 7.5 MB/s \n","\u001b[?25hCollecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 57.5 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n","\u001b[K     |████████████████████████████████| 139 kB 58.8 MB/s \n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting configparser>=3.8.1\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Collecting yaspin>=1.0.0\n","  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=724fd9670c8adb787a766328cc07865a7982586a42331717dc4b36bc90766bf1\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=6a3cf6b59ffe64c58f934e5ad16bf361f071fbb0e9c32cf0102e33810f99fbac\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.4.3 shortuuid-1.0.1 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.6 yaspin-2.1.0\n"]}],"source":["!pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"beCCkJ6DQBtf"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2185,"status":"ok","timestamp":1635401197093,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"GweU62UIQIqJ","outputId":"8e9df4a0-3866-4c7a-fde4-a87ccc3192ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import regex\n","from tqdm import tqdm\n","\n","from sklearn import preprocessing, metrics\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n","from torch.utils.data import Dataset, DataLoader\n","\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow as tf\n","import transformers\n","from transformers import XLNetForSequenceClassification, XLNetTokenizer, TFXLNetModel, XLNetModel, AdamW, get_linear_schedule_with_warmup\n","from transformers import RobertaModel, RobertaTokenizer\n","\n","import nltk\n","from nltk.corpus import stopwords \n","from nltk.tokenize import WordPunctTokenizer\n","from string import punctuation\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1635401197094,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"cupnx6q_vNb2","outputId":"4d02fb9c-0daa-4a19-9f34-d02eccf5a8d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.9.0+cu111\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tayL3He7uvF8"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_9qOO8LiN2p"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"0KgP7rRLQFZO"},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfutfGKBQAlH"},"outputs":[],"source":["f_path = '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/dataset/crowdflower_text_emotion.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgpvlBcMQAnH"},"outputs":[],"source":["df = pd.read_csv(f_path)"]},{"cell_type":"markdown","metadata":{"id":"OuzN46oCQMOc"},"source":["### Data preprocessing and analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1635401199256,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"oGKcv1-GQApG","outputId":"c7511ab2-4cb1-4445-8c27-ce10fbc9e582"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id  ...                                            content\n","0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n","1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696  ...                Funeral ceremony...gloomy friday...\n","3  1956967789  ...               wants to hang out with friends SOON!\n","4  1956968416  ...  @dannycastillo We want to trade with someone w...\n","\n","[5 rows x 4 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1635401199256,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"T3uI-NOsQArQ","outputId":"ce91e18d-e98c-49f8-c0db-d4107183080f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n"," 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"]}],"source":["# Check emotions labels\n","\n","labels = df.sentiment.unique()\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KG7DyxzPQAtI"},"outputs":[],"source":["# Remove irrelevant columns\n","df = df.drop(columns=['tweet_id', 'author'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1635401199656,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"IA3PHfffNONe","outputId":"9d5188bf-e48a-45d1-82ca-7146d3a8edf1"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Check for null cells\n","df['sentiment'].isnull().values.any()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lw6l6A1i7vUk"},"outputs":[],"source":["# Remove stop words, lemmatize, clean text\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","stop = stopwords.words('english')\n","\n","for punct in punctuation:\n","    stop.append(punct)\n","\n","def clean(x, stop_words):\n","    word_tokens = WordPunctTokenizer().tokenize(x.lower())\n","    x = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 2]\n","    return \" \".join(x)\n","\n","df[\"content_cleaned\"] = df.content.apply(lambda x : clean(x, stop)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNrhUEtWNOPz"},"outputs":[],"source":["# Check len of each content\n","df['content_count'] = df['content_cleaned'].str.split(\" \").str.len()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1635401202041,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"BKJ0zVzG_4Di","outputId":"d3f7a480-e6a7-4752-bb35-0f8f4e4aea47"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","      <th>content_cleaned</th>\n","      <th>content_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>empty</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>tiffanylue know was listenin bad habit earlier...</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>layin bed with headache ughhhh waitin your call</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>wants hang out with friends soon</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>dannycastillo want trade with someone who has ...</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    sentiment  ... content_count\n","0       empty  ...            12\n","1     sadness  ...             8\n","2     sadness  ...             4\n","3  enthusiasm  ...             6\n","4     neutral  ...            12\n","\n","[5 rows x 4 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"RJ9ZRe_4BGGf"},"source":["### Label encoding classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdPWKgwWF_d-"},"outputs":[],"source":["labels_dict = dict(zip(labels, (x for x in range(len(labels)) )))\n","df['sentiment_val'] = df['sentiment'].map(labels_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLvTvEDeAggz"},"outputs":[],"source":["df = df.drop(columns=['sentiment', 'content_count', 'content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1635401202041,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"phXezD-LAqab","outputId":"4d0ffefb-a79f-45f9-a341-a21011b0bfac"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content_cleaned</th>\n","      <th>sentiment_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tiffanylue know was listenin bad habit earlier...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>layin bed with headache ughhhh waitin your call</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>wants hang out with friends soon</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>dannycastillo want trade with someone who has ...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     content_cleaned  sentiment_val\n","0  tiffanylue know was listenin bad habit earlier...              0\n","1    layin bed with headache ughhhh waitin your call              1\n","2                     funeral ceremony gloomy friday              1\n","3                   wants hang out with friends soon              2\n","4  dannycastillo want trade with someone who has ...              3"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"Iz3SZaTNYQQM"},"source":["### Prep data for training/validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KuaiJhtENUQ"},"outputs":[],"source":["class SentimentData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.content_cleaned\n","        self.targets = self.data.sentiment_val\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4xJ5o_xiXM1"},"outputs":[],"source":["# Batch Size\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YULG6eb_KjTz"},"outputs":[],"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 2\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 2\n","                }"]},{"cell_type":"markdown","metadata":{"id":"Vl9Ax3Nw_BZn"},"source":["### BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbbILdHfEmMy"},"outputs":[],"source":["from transformers import BertTokenizerFast, BertForSequenceClassification, AutoModel, BertConfig, BertModel"]},{"cell_type":"markdown","metadata":{"id":"e7YZ3hH_Ta6X"},"source":["#### Base-cased"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248,"referenced_widgets":["3a0e69593c834849a903eb2649b8da6a","f802ed9281864ac28f8c02b954d3da7c","d12f4c5bda08448f956c1afdf7d90912","10b2e7dc57f940b3ae5e650b80756ab1","f4ad6364e86e4572bd01b5e8e6851dda","d87d001bbed04b3397a61993c88b6c1d","90799d58daec47c2926669280ab2967a","f764a1a017ff460a903a3d85c8703a36","29b4c8bc1ab141cea52ecaf377bcfe93","b0468c2dbcde4f6ca5b44059b0612e42","bae7c1ac09494620b09b49f1bb3bad27","5fc1709b0c9c470d9f47349eb64b1db7","006abffc60fd4241b14a82a3efc2a7ee","4eed8fcdd49c42d098b65784dc46118a","a2b0fbb6f0a44deab46e86db187c206d","e2db1802125445b79cb5343fd71174c5","e00d100b5ad043cbb894f3d6bc0b693e","3244445761ac4b11a0177cdcf285c3d2","a2b5d5bef80344d5976c0d9cf8e0b355","5ff1bf466f964730be4fae6fcf2e8c0f","c17685476a474258829a252061a5dd04","f383cbec75ca4b82ba2ecf5ac26eacde","2df6dc87084049678ec44b2450650ec4","4f76ba1b019848948980e82765436db5","5c260997426c48848d67da0fd6508a97","fe1cf676a14744ada20052bedbd51dfc","b6bc3651e9af4a819b12c44b185dcb2b","f0864626fda24535bd0e7e52478caf3c","6068e6ee47284ea2b480d11d9226b986","46c9ad77f0dc4ebaa07ace69d0c35999","f1bbc0973bfc4ff2abb57f8f025baab6","40abb515983244b3a80ca73fa4d30850","45bc5af053c94d3990a2f06b900dcf12","304c9fd3e9dd439893d64779883704df","3b6b885f51dd4354ac8add87389df5dc","7b903b61d95b48a5a6423259a14f28af","59d7122bafa748ac812041d8f0816e35","4fa248e0d13b4aeebfc5381902c2a848","74436af4d12943bba4c3fa060167e27f","e798182fd8cb4bac877ac8185c9798f8","f9064b53c7b24d43b2141224384c3607","7cc82a03a9544876bb86b8199a101e8b","6e341234088c43f4aa643ea05a88ff04","121c319e22c442f0a53862ed78029949","1be4692e73f44bfc961bba8d8254f5fc","89e1eae75e6842b68094504f1f8bab50","af5eca6801c449539511d0df041fcf32","2a8d87095d7f46a0b4f7da78d9d479bf","93a4f672ee604bf0beaa825dedf5787a","34dfc789309146948911cbe7c1b33cb3","4f86608c7cec44248f527eb6b5f1c961","11466598784a413493b96f7db7984316","e29badfbbd074e9f80737ed042e4b6e9","d316926e597c49a9bb589fb015c4c358","f6c6b7d4c4474507b1577f6ebeff98e8"]},"executionInfo":{"elapsed":16424,"status":"ok","timestamp":1635401219008,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"g_OtM1y1_A1w","outputId":"d5c12e46-8c5f-4889-82cf-2db365555b86"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a0e69593c834849a903eb2649b8da6a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fc1709b0c9c470d9f47349eb64b1db7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2df6dc87084049678ec44b2450650ec4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"304c9fd3e9dd439893d64779883704df","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1be4692e73f44bfc961bba8d8254f5fc","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n","bert = AutoModel.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNmB_rer_DYg"},"outputs":[],"source":["class BERT(nn.Module):\n","\n","    def __init__(self, config):\n","        super(BERT, self).__init__()\n","        self.num_labels = config.num_labels\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n","                position_ids=None, head_mask=None):s\n","      \n","        # Forward pass through pre-trained BERT\n","        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n","                            attention_mask=attention_mask, head_mask=head_mask)\n","\n","        pooler = outputs[-1]\n","        pooler = self.pre_classifier(pooler\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        \n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryim5oamQHgC"},"outputs":[],"source":["config = BertConfig(hidden_size=768,\n","                    hidden_dropout_prob=0.3,\n","                    num_labels=13)"]},{"cell_type":"markdown","metadata":{"id":"yJR_LDAQNjaR"},"source":["### Train/Eval functions"]},{"cell_type":"markdown","metadata":{"id":"6sBbhhzJiENf"},"source":["#### Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkXQL8QDiDwh"},"outputs":[],"source":["LEARNING_RATE = 1e-05\n","MAX_LEN = 256\n","\n","# Multi class loss\n","loss_function = torch.nn.CrossEntropyLoss()\n","\n","# Accuracy function\n","def calcuate_accuracy(preds, targets):\n","    n_correct = (preds==targets).sum().item()\n","    return n_correct"]},{"cell_type":"markdown","metadata":{"id":"hpDp_VFKjI3_"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWl0UERQi_3C"},"outputs":[],"source":["# For BERT base\n","def train(model, epoch, training_loader):\n","\n","  tr_loss = 0\n","  n_correct = 0\n","  nb_tr_steps = 0\n","  nb_tr_examples = 0\n","\n","  # Tell wandb to watch this model train\n","  # wandb.watch(model)\n","\n","  model.train()\n","\n","  for _, data in tqdm(enumerate(training_loader, 0)):\n","\n","    ids = data['ids'].to(device, dtype = torch.long)\n","    mask = data['mask'].to(device, dtype = torch.long)\n","    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","    targets = data['targets'].to(device, dtype = torch.long)\n","\n","    outputs = model(ids, mask, token_type_ids)\n","    loss = loss_function(outputs, targets)\n","    tr_loss += loss.item()\n","    big_val, big_idx = torch.max(outputs.data, dim=1)\n","    n_correct += calcuate_accuracy(big_idx, targets)\n","\n","    nb_tr_steps += 1\n","    nb_tr_examples+=targets.size(0)\n","    \n","    if _%5000==0:\n","        loss_step = tr_loss/nb_tr_steps\n","        accu_step = (n_correct*100)/nb_tr_examples \n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Optimizer \n","    optimizer.step()\n","\n","  print('\\n')\n","  epoch_loss = tr_loss/nb_tr_steps\n","  epoch_accu = (n_correct*100)/nb_tr_examples\n","  print(f\"Training Loss Epoch {epoch+1}: {epoch_loss}\")\n","  print(f\"Training Accuracy Epoch {epoch+1}: {epoch_accu} \\n\")\n","  \n","  return epoch_accu, epoch_loss"]},{"cell_type":"markdown","metadata":{"id":"5Gpk3QhujbD2"},"source":["#### Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGGf2WT9jKyS"},"outputs":[],"source":["def valid(model, testing_loader):\n","\n","    n_correct = 0; \n","    n_wrong = 0; \n","    total = 0; \n","    tr_loss=0; \n","    nb_tr_steps=0; \n","    nb_tr_examples=0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for _, data in tqdm(enumerate(testing_loader, 0)):\n","\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype = torch.long)\n","            outputs = model(ids, mask, token_type_ids).squeeze()\n","            loss = loss_function(outputs, targets)\n","            tr_loss += loss.item()\n","            big_val, big_idx = torch.max(outputs.data, dim=1)\n","            n_correct += calcuate_accuracy(big_idx, targets)\n","\n","            nb_tr_steps += 1\n","            nb_tr_examples+=targets.size(0)\n","            \n","            if _%5000==0:\n","                loss_step = tr_loss/nb_tr_steps\n","                accu_step = (n_correct*100)/nb_tr_examples\n","        # Reduce LR on val loss plateau\n","        scheduler.step(loss)\n","\n","    print('\\n')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Validation Loss Epoch : {epoch_loss}\")\n","    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n","    \n","    return epoch_accu, epoch_loss\n"]},{"cell_type":"markdown","metadata":{"id":"g4wUnrjUHsSq"},"source":["### K-Fold (BERT base-cased)"]},{"cell_type":"markdown","metadata":{"id":"A8DvAPvIRCUj"},"source":["https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n","https://www.kaggle.com/aaybeedee/transfer-learning-for-text-data-in-pytorch-bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYqaYA4fjK0R"},"outputs":[],"source":["# Set K-fold\n","kf = KFold(n_splits=3, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":6597,"status":"ok","timestamp":1635401261059,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"MqVdGDcavQpa","outputId":"bac313a3-4f4b-4fc7-a522-acc85eff4ec9"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Setup wandb for logging\n","# import wandb\n","\n","# wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d120276b75c5453fb1069822c2131bfb","8822e2875f144147934f8d3c69e0994d","18434fccb80441328ca9ab7128e7a2b9","05454c676bae43ec89091caf6cbff782","c9bccdac02c243b4aa84c49cb45c6fc6","42e3dc74b787406f81d046a001b56566","ee05b17e54594bd497b08cb185cccbe9","856f3d562af54d558f161973866a517a","15f4d9e0a2a4406a8f4e24a3d2a7ce9c","f4c901cdb32b4c05a9c06ad22eb85e60","9815464479b74da78d73804c97b3ed80","11d4a9d37a59463e8b4d301ccef31cad","8e0fd4db7ac4456dbaedea01bacf539c","791db0535fa34461900203dea56c57eb","369ed49a388344c693936fdcccbe8f3c","80d4b1baba8b41589c5b1310712ee94d","648c055faf7b4537bbf56b45b67f0581","2510afb41b51445dabdececeba249ad0","afaa074ab9b742868e6b7ecc5f5d3de5","b8cd501b6da5494ca2b3a6f3810e420a","049cd7df28a945a1bcf025c1a5c21d16","317e873dcc7848b09c7aa3b585a14f05","344226aea58c496483c71ddc13f4a945","687fad1c6a734dc9a0c040165a6a9c80"]},"executionInfo":{"elapsed":38992497,"status":"ok","timestamp":1635440270093,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"KEfTFkBEGv6J","outputId":"44884b10-1f9e-4633-f915-a5616f8dcc42"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwasabee\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/b4veswlw\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 1...\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:18,  4.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 2.2736193073961504\n","Training Accuracy Epoch 1: 19.204230105752643 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.3499453205796295\n","Validation Accuracy Epoch: 10.866956652167392\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:18,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 2.2293689975355226\n","Training Accuracy Epoch 2: 19.657991449786245 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.330299962975697\n","Validation Accuracy Epoch: 10.866956652167392\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:20,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 2.203918267812426\n","Training Accuracy Epoch 3: 20.798019950498762 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.108491423677812\n","Validation Accuracy Epoch: 32.20338983050848\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:20,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 2.1174817327331765\n","Training Accuracy Epoch 4: 24.3793594839871 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.0474242284116304\n","Validation Accuracy Epoch: 33.958302084895756\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:21,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 2.0590381813249548\n","Training Accuracy Epoch 5: 26.678166954173854 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9837789234460008\n","Validation Accuracy Epoch: 28.40857957102145\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:22,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 2.000924560540296\n","Training Accuracy Epoch 6: 29.505737643441087 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9493737072378272\n","Validation Accuracy Epoch: 35.143242837858104\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:22,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.9568758184636648\n","Training Accuracy Epoch 7: 32.329558238955975 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9542041079470263\n","Validation Accuracy Epoch: 33.718314084295784\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:23,  4.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 1.914737349759338\n","Training Accuracy Epoch 8: 33.93084827120678 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.8720604719769547\n","Validation Accuracy Epoch: 38.75806209689515\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:25,  4.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 1.8831357131765214\n","Training Accuracy Epoch 9: 35.60339008475212 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9169109057865246\n","Validation Accuracy Epoch: 35.96070196490175\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:26,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 1.842285605233518\n","Training Accuracy Epoch 10: 36.92342308557714 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.785425808448311\n","Validation Accuracy Epoch: 42.49287535623219\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:26,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 1.8000890961219682\n","Training Accuracy Epoch 11: 38.53221330533263 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.763385837601557\n","Validation Accuracy Epoch: 43.58782060896955\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:27,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 1.7589174090886301\n","Training Accuracy Epoch 12: 40.373509337733445 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7579599492336793\n","Validation Accuracy Epoch: 44.652767361631916\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:28,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 1.71616444803314\n","Training Accuracy Epoch 13: 41.88104702617566 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7619168626096482\n","Validation Accuracy Epoch: 45.19274036298185\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:29,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 1.6702336352173268\n","Training Accuracy Epoch 14: 43.3998349958749 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7419691409761489\n","Validation Accuracy Epoch: 46.017699115044245\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:31,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 1.6262374872316052\n","Training Accuracy Epoch 15: 44.77236930923273 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.6540537287213521\n","Validation Accuracy Epoch: 48.33508324583771\n","Epoch 15 done!\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 376... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d120276b75c5453fb1069822c2131bfb","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.36MB of 0.36MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Acc</td><td>▁▁▁▂▃▄▅▅▅▆▆▇▇██</td></tr><tr><td>Train_Loss</td><td>██▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>Val_Acc</td><td>▁▁▅▅▄▆▅▆▆▇▇▇▇██</td></tr><tr><td>Val_Loss</td><td>██▆▅▄▄▄▃▄▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>Train_Acc</td><td>44.77237</td></tr><tr><td>Train_Loss</td><td>1.62624</td></tr><tr><td>Val_Acc</td><td>48.33508</td></tr><tr><td>Val_Loss</td><td>1.65405</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">3-Fold</strong>: <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/b4veswlw\" target=\"_blank\">https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/b4veswlw</a><br/>\n","Find logs at: <code>./wandb/run-20211028_060812-b4veswlw/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/2prkyqd4\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 2...\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:18,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 2.277059441994391\n","Training Accuracy Epoch 1: 19.57100536243297 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.2026453548086997\n","Validation Accuracy Epoch: 23.48308707717693\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:19,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 2.234998323599879\n","Training Accuracy Epoch 2: 19.492256346795664 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.57it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.320203678723789\n","Validation Accuracy Epoch: 23.48308707717693\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:20,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 2.196555776086909\n","Training Accuracy Epoch 3: 21.464731690853863 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.211837024479908\n","Validation Accuracy Epoch: 17.985449636240904\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:19,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 2.1064446651799704\n","Training Accuracy Epoch 4: 24.955938050774364 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:01, 13.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.019179154457843\n","Validation Accuracy Epoch: 34.26085652141303\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:17,  4.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 2.0657729868339647\n","Training Accuracy Epoch 5: 26.230922113473582 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.0782358588111136\n","Validation Accuracy Epoch: 24.285607140178506\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:17,  4.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 2.031546133753062\n","Training Accuracy Epoch 6: 27.03341208234897 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.089282087852087\n","Validation Accuracy Epoch: 30.360759018975475\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:18,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.997453365968814\n","Training Accuracy Epoch 7: 28.390895113811077 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9228944507057109\n","Validation Accuracy Epoch: 37.5534388359709\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:18,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 1.927573975611391\n","Training Accuracy Epoch 8: 31.919601004987438 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9551891348071633\n","Validation Accuracy Epoch: 34.80087002175054\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:19,  4.51it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 1.9184015732578124\n","Training Accuracy Epoch 9: 32.234597067536654 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9631129980230302\n","Validation Accuracy Epoch: 34.32835820895522\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:20,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 1.9108102053815044\n","Training Accuracy Epoch 10: 33.089586380170246 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9849994435379015\n","Validation Accuracy Epoch: 32.220805520138\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:21,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 1.9016589334108334\n","Training Accuracy Epoch 11: 33.34458319271009 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.0146563248356877\n","Validation Accuracy Epoch: 31.958298957473936\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:22,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 1.892781948910978\n","Training Accuracy Epoch 12: 33.670829114636064 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9750689528770766\n","Validation Accuracy Epoch: 33.188329708242705\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:22,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 1.8953672838053734\n","Training Accuracy Epoch 13: 33.727078411519855 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 14.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9806379015458582\n","Validation Accuracy Epoch: 33.203330083252084\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:23,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 1.8903171808761112\n","Training Accuracy Epoch 14: 34.094573817827275 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.974818834732161\n","Validation Accuracy Epoch: 33.3758343958599\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:24,  4.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 1.893128556785429\n","Training Accuracy Epoch 15: 34.02707466156673 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.973217305946865\n","Validation Accuracy Epoch: 33.17332933323333\n","Epoch 15 done!\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 2021... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15f4d9e0a2a4406a8f4e24a3d2a7ce9c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.40MB of 0.40MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Acc</td><td>▁▁▂▄▄▅▅▇▇██████</td></tr><tr><td>Train_Loss</td><td>█▇▇▅▄▄▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Val_Acc</td><td>▃▃▁▇▃▅█▇▇▆▆▆▆▇▆</td></tr><tr><td>Val_Loss</td><td>▆█▆▃▄▄▁▂▂▂▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>Train_Acc</td><td>34.02707</td></tr><tr><td>Train_Loss</td><td>1.89313</td></tr><tr><td>Val_Acc</td><td>33.17333</td></tr><tr><td>Val_Loss</td><td>1.97322</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">3-Fold</strong>: <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/2prkyqd4\" target=\"_blank\">https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/2prkyqd4</a><br/>\n","Find logs at: <code>./wandb/run-20211028_094526-2prkyqd4/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/3sozvg6a\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 3...\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:13,  4.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 2.25660127319114\n","Training Accuracy Epoch 1: 19.676004049949377 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.2229099235780665\n","Validation Accuracy Epoch: 23.48308707717693\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:14,  4.54it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 2.2265317662433013\n","Training Accuracy Epoch 2: 20.174747815652303 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.397835365440149\n","Validation Accuracy Epoch: 10.867771694292358\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:15,  4.53it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 2.1986375999221845\n","Training Accuracy Epoch 3: 21.20973487831402 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.96it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.11504029433886\n","Validation Accuracy Epoch: 22.335558388959726\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:15,  4.53it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 2.1102598138962523\n","Training Accuracy Epoch 4: 24.783440206997412 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:58, 14.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 2.054944879435177\n","Validation Accuracy Epoch: 26.588164704117602\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:17,  4.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 2.0617095862381745\n","Training Accuracy Epoch 5: 26.107173660329245 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.960004807662735\n","Validation Accuracy Epoch: 35.933398334958376\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:17,  4.52it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 2.020225858502425\n","Training Accuracy Epoch 6: 27.74215322308471 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9671721195988692\n","Validation Accuracy Epoch: 35.708392709817744\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:21,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.9737286069611983\n","Training Accuracy Epoch 7: 30.70836614542318 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.984795994828687\n","Validation Accuracy Epoch: 33.278331958298956\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:20,  4.50it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 1.9021286346737896\n","Training Accuracy Epoch 8: 33.7420782240222 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9277493356943274\n","Validation Accuracy Epoch: 35.60339008475212\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:27,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 1.8868695134771893\n","Training Accuracy Epoch 9: 35.092061349233134 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:04, 13.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9279036970287293\n","Validation Accuracy Epoch: 35.3708842721068\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:33,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 1.8823967781264266\n","Training Accuracy Epoch 10: 35.21580980237747 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:04, 13.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.937133059540264\n","Validation Accuracy Epoch: 35.99339983499588\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:32,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 1.8743789896467309\n","Training Accuracy Epoch 11: 35.41080736490794 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:04, 13.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9403244851947046\n","Validation Accuracy Epoch: 35.41588539713493\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:31,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 1.8642866349928238\n","Training Accuracy Epoch 12: 35.82705216184797 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9364192956711048\n","Validation Accuracy Epoch: 35.46088652216305\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:29,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 1.8631557530997824\n","Training Accuracy Epoch 13: 36.11204859939251 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:02, 13.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.9266681646113633\n","Validation Accuracy Epoch: 36.34590864771619\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:30,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 1.8492032577159094\n","Training Accuracy Epoch 14: 36.32954588067649 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.932093887788204\n","Validation Accuracy Epoch: 35.678391959798994\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:35,  4.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 1.8463037313997352\n","Training Accuracy Epoch 15: 36.547043161960474 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:03, 13.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.934484794494558\n","Validation Accuracy Epoch: 35.49838745968649\n","Epoch 15 done!\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 3050... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"648c055faf7b4537bbf56b45b67f0581","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.44MB of 0.44MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Acc</td><td>▁▁▂▃▄▄▆▇▇▇█████</td></tr><tr><td>Train_Loss</td><td>█▇▇▆▅▄▃▂▂▂▁▁▁▁▁</td></tr><tr><td>Val_Acc</td><td>▄▁▄▅██▇████████</td></tr><tr><td>Val_Loss</td><td>▅█▄▃▁▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>Train_Acc</td><td>36.54704</td></tr><tr><td>Train_Loss</td><td>1.8463</td></tr><tr><td>Val_Acc</td><td>35.49839</td></tr><tr><td>Val_Loss</td><td>1.93448</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">3-Fold</strong>: <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/3sozvg6a\" target=\"_blank\">https://wandb.ai/wasabee/Text_Emotion_Recognition_BERT_base_cased2/runs/3sozvg6a</a><br/>\n","Find logs at: <code>./wandb/run-20211028_132104-3sozvg6a/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["EPOCHS=15\n","f = 1\n","df = df.reset_index(drop=True)\n","\n","for train_index, val_index in kf.split(df):\n","\n","  # Init model\n","  model = BERT(config)\n","  # push the model to GPU\n","  model = model.to(device)\n","  # Optimizer, need to define after init model so it doesnt throw error\n","  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n","  # Reduce LR on plateau\n","  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n","\n","  # Init wandb for logging\n","  # wandb.init(project=\"Text_Emotion_Recognition_BERT_base_cased2\", name=f\"3-Fold\")\n","\n","  train2 = df.iloc[train_index]\n","  test2 = df.iloc[val_index]\n","\n","  train2 = train2.reset_index(drop=True)\n","  test2 = df.drop(train2.index).reset_index(drop=True)\n","\n","  training_set = SentimentData(train2, tokenizer, MAX_LEN)\n","  testing_set = SentimentData(test2, tokenizer, MAX_LEN)\n","\n","  training_loader = DataLoader(training_set, **train_params)\n","  testing_loader = DataLoader(testing_set, **test_params)\n","\n","  print(f\"Training fold {f}...\")\n","\n","  for epoch in range(0, EPOCHS):\n","    \n","    # Train\n","    train_acc, train_loss = train(model, epoch, training_loader)\n","    # Val\n","    val_acc, val_loss = valid(model, testing_loader)\n","    print(f\"Epoch {epoch+1} done!\")\n","    # Log metrics\n","    # wandb.log({\"Train_Loss\": train_loss, \"Val_Loss\": val_loss, \"Train_Acc\": train_acc, \"Val_Acc\": val_acc, \"Epoch\": epoch})   \n","\n","  # wandb.finish()\n","  f+=1"]},{"cell_type":"markdown","metadata":{"id":"jUfUWS5-It4a"},"source":["### Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2985,"status":"ok","timestamp":1635440273075,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"X9PXHnI-IowG","outputId":"c06c7edd-1b5a-4757-92fa-9ec72be83e72"},"outputs":[{"data":{"text/plain":["('/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/bert_base2/vocab.txt',)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Base model\n","bert_base = model\n","torch.save(bert_base, '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/bert_base2/bert_sent_base.bin')\n","tokenizer.save_vocabulary('/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/bert_base2/')\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOncynSfEuZRDFXhDDwg8ZF","collapsed_sections":["OW9pCCPaQJa6","beCCkJ6DQBtf","0KgP7rRLQFZO","OuzN46oCQMOc","RJ9ZRe_4BGGf","Iz3SZaTNYQQM","Vl9Ax3Nw_BZn","e7YZ3hH_Ta6X","yJR_LDAQNjaR","hpDp_VFKjI3_","5Gpk3QhujbD2","g4wUnrjUHsSq","jUfUWS5-It4a"],"name":"BERT_base.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"006abffc60fd4241b14a82a3efc2a7ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049cd7df28a945a1bcf025c1a5c21d16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05454c676bae43ec89091caf6cbff782":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_856f3d562af54d558f161973866a517a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee05b17e54594bd497b08cb185cccbe9","value":1}},"10b2e7dc57f940b3ae5e650b80756ab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_29b4c8bc1ab141cea52ecaf377bcfe93","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f764a1a017ff460a903a3d85c8703a36","value":213450}},"11466598784a413493b96f7db7984316":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11d4a9d37a59463e8b4d301ccef31cad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_80d4b1baba8b41589c5b1310712ee94d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_369ed49a388344c693936fdcccbe8f3c","value":1}},"121c319e22c442f0a53862ed78029949":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15f4d9e0a2a4406a8f4e24a3d2a7ce9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_9815464479b74da78d73804c97b3ed80","IPY_MODEL_11d4a9d37a59463e8b4d301ccef31cad"],"layout":"IPY_MODEL_f4c901cdb32b4c05a9c06ad22eb85e60"}},"18434fccb80441328ca9ab7128e7a2b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42e3dc74b787406f81d046a001b56566","placeholder":"​","style":"IPY_MODEL_c9bccdac02c243b4aa84c49cb45c6fc6","value":" 6.12MB of 6.12MB uploaded (0.00MB deduped)\r"}},"1be4692e73f44bfc961bba8d8254f5fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af5eca6801c449539511d0df041fcf32","IPY_MODEL_2a8d87095d7f46a0b4f7da78d9d479bf","IPY_MODEL_93a4f672ee604bf0beaa825dedf5787a"],"layout":"IPY_MODEL_89e1eae75e6842b68094504f1f8bab50"}},"2510afb41b51445dabdececeba249ad0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29b4c8bc1ab141cea52ecaf377bcfe93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a8d87095d7f46a0b4f7da78d9d479bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e29badfbbd074e9f80737ed042e4b6e9","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11466598784a413493b96f7db7984316","value":435779157}},"2df6dc87084049678ec44b2450650ec4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c260997426c48848d67da0fd6508a97","IPY_MODEL_fe1cf676a14744ada20052bedbd51dfc","IPY_MODEL_b6bc3651e9af4a819b12c44b185dcb2b"],"layout":"IPY_MODEL_4f76ba1b019848948980e82765436db5"}},"304c9fd3e9dd439893d64779883704df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b903b61d95b48a5a6423259a14f28af","IPY_MODEL_59d7122bafa748ac812041d8f0816e35","IPY_MODEL_4fa248e0d13b4aeebfc5381902c2a848"],"layout":"IPY_MODEL_3b6b885f51dd4354ac8add87389df5dc"}},"317e873dcc7848b09c7aa3b585a14f05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3244445761ac4b11a0177cdcf285c3d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"344226aea58c496483c71ddc13f4a945":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34dfc789309146948911cbe7c1b33cb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"369ed49a388344c693936fdcccbe8f3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a0e69593c834849a903eb2649b8da6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d12f4c5bda08448f956c1afdf7d90912","IPY_MODEL_10b2e7dc57f940b3ae5e650b80756ab1","IPY_MODEL_f4ad6364e86e4572bd01b5e8e6851dda"],"layout":"IPY_MODEL_f802ed9281864ac28f8c02b954d3da7c"}},"3b6b885f51dd4354ac8add87389df5dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40abb515983244b3a80ca73fa4d30850":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42e3dc74b787406f81d046a001b56566":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45bc5af053c94d3990a2f06b900dcf12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46c9ad77f0dc4ebaa07ace69d0c35999":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4eed8fcdd49c42d098b65784dc46118a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3244445761ac4b11a0177cdcf285c3d2","placeholder":"​","style":"IPY_MODEL_e00d100b5ad043cbb894f3d6bc0b693e","value":"Downloading: 100%"}},"4f76ba1b019848948980e82765436db5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f86608c7cec44248f527eb6b5f1c961":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa248e0d13b4aeebfc5381902c2a848":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_121c319e22c442f0a53862ed78029949","placeholder":"​","style":"IPY_MODEL_6e341234088c43f4aa643ea05a88ff04","value":" 570/570 [00:00&lt;00:00, 13.0kB/s]"}},"59d7122bafa748ac812041d8f0816e35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cc82a03a9544876bb86b8199a101e8b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9064b53c7b24d43b2141224384c3607","value":570}},"5c260997426c48848d67da0fd6508a97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6068e6ee47284ea2b480d11d9226b986","placeholder":"​","style":"IPY_MODEL_f0864626fda24535bd0e7e52478caf3c","value":"Downloading: 100%"}},"5fc1709b0c9c470d9f47349eb64b1db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4eed8fcdd49c42d098b65784dc46118a","IPY_MODEL_a2b0fbb6f0a44deab46e86db187c206d","IPY_MODEL_e2db1802125445b79cb5343fd71174c5"],"layout":"IPY_MODEL_006abffc60fd4241b14a82a3efc2a7ee"}},"5ff1bf466f964730be4fae6fcf2e8c0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6068e6ee47284ea2b480d11d9226b986":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"648c055faf7b4537bbf56b45b67f0581":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_afaa074ab9b742868e6b7ecc5f5d3de5","IPY_MODEL_b8cd501b6da5494ca2b3a6f3810e420a"],"layout":"IPY_MODEL_2510afb41b51445dabdececeba249ad0"}},"687fad1c6a734dc9a0c040165a6a9c80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e341234088c43f4aa643ea05a88ff04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74436af4d12943bba4c3fa060167e27f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"791db0535fa34461900203dea56c57eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b903b61d95b48a5a6423259a14f28af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e798182fd8cb4bac877ac8185c9798f8","placeholder":"​","style":"IPY_MODEL_74436af4d12943bba4c3fa060167e27f","value":"Downloading: 100%"}},"7cc82a03a9544876bb86b8199a101e8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80d4b1baba8b41589c5b1310712ee94d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"856f3d562af54d558f161973866a517a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8822e2875f144147934f8d3c69e0994d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89e1eae75e6842b68094504f1f8bab50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e0fd4db7ac4456dbaedea01bacf539c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90799d58daec47c2926669280ab2967a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a4f672ee604bf0beaa825dedf5787a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6c6b7d4c4474507b1577f6ebeff98e8","placeholder":"​","style":"IPY_MODEL_d316926e597c49a9bb589fb015c4c358","value":" 416M/416M [00:10&lt;00:00, 42.7MB/s]"}},"9815464479b74da78d73804c97b3ed80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_791db0535fa34461900203dea56c57eb","placeholder":"​","style":"IPY_MODEL_8e0fd4db7ac4456dbaedea01bacf539c","value":" 6.18MB of 6.18MB uploaded (0.00MB deduped)\r"}},"a2b0fbb6f0a44deab46e86db187c206d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff1bf466f964730be4fae6fcf2e8c0f","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2b5d5bef80344d5976c0d9cf8e0b355","value":435797}},"a2b5d5bef80344d5976c0d9cf8e0b355":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af5eca6801c449539511d0df041fcf32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f86608c7cec44248f527eb6b5f1c961","placeholder":"​","style":"IPY_MODEL_34dfc789309146948911cbe7c1b33cb3","value":"Downloading: 100%"}},"afaa074ab9b742868e6b7ecc5f5d3de5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_317e873dcc7848b09c7aa3b585a14f05","placeholder":"​","style":"IPY_MODEL_049cd7df28a945a1bcf025c1a5c21d16","value":" 6.22MB of 6.22MB uploaded (0.00MB deduped)\r"}},"b0468c2dbcde4f6ca5b44059b0612e42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6bc3651e9af4a819b12c44b185dcb2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45bc5af053c94d3990a2f06b900dcf12","placeholder":"​","style":"IPY_MODEL_40abb515983244b3a80ca73fa4d30850","value":" 29.0/29.0 [00:00&lt;00:00, 678B/s]"}},"b8cd501b6da5494ca2b3a6f3810e420a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_687fad1c6a734dc9a0c040165a6a9c80","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_344226aea58c496483c71ddc13f4a945","value":1}},"bae7c1ac09494620b09b49f1bb3bad27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c17685476a474258829a252061a5dd04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9bccdac02c243b4aa84c49cb45c6fc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d120276b75c5453fb1069822c2131bfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_18434fccb80441328ca9ab7128e7a2b9","IPY_MODEL_05454c676bae43ec89091caf6cbff782"],"layout":"IPY_MODEL_8822e2875f144147934f8d3c69e0994d"}},"d12f4c5bda08448f956c1afdf7d90912":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90799d58daec47c2926669280ab2967a","placeholder":"​","style":"IPY_MODEL_d87d001bbed04b3397a61993c88b6c1d","value":"Downloading: 100%"}},"d316926e597c49a9bb589fb015c4c358":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d87d001bbed04b3397a61993c88b6c1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e00d100b5ad043cbb894f3d6bc0b693e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e29badfbbd074e9f80737ed042e4b6e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2db1802125445b79cb5343fd71174c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f383cbec75ca4b82ba2ecf5ac26eacde","placeholder":"​","style":"IPY_MODEL_c17685476a474258829a252061a5dd04","value":" 426k/426k [00:00&lt;00:00, 867kB/s]"}},"e798182fd8cb4bac877ac8185c9798f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee05b17e54594bd497b08cb185cccbe9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0864626fda24535bd0e7e52478caf3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1bbc0973bfc4ff2abb57f8f025baab6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f383cbec75ca4b82ba2ecf5ac26eacde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4ad6364e86e4572bd01b5e8e6851dda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bae7c1ac09494620b09b49f1bb3bad27","placeholder":"​","style":"IPY_MODEL_b0468c2dbcde4f6ca5b44059b0612e42","value":" 208k/208k [00:00&lt;00:00, 607kB/s]"}},"f4c901cdb32b4c05a9c06ad22eb85e60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6c6b7d4c4474507b1577f6ebeff98e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f764a1a017ff460a903a3d85c8703a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f802ed9281864ac28f8c02b954d3da7c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9064b53c7b24d43b2141224384c3607":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe1cf676a14744ada20052bedbd51dfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1bbc0973bfc4ff2abb57f8f025baab6","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46c9ad77f0dc4ebaa07ace69d0c35999","value":29}}}}},"nbformat":4,"nbformat_minor":0}
