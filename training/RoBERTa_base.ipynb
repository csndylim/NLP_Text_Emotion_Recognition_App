{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20689,"status":"ok","timestamp":1635442425893,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"v1L-D4onRFZr","outputId":"40ae6554-879f-47c9-b10d-fd77d77a02e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OW9pCCPaQJa6"},"source":["### Download dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6089,"status":"ok","timestamp":1635442431978,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"wjcAMvXqtRSn","outputId":"67c9632f-b34b-4c68-f520-64ec27899a3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 62.0 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 65.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 55.0 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0\n"]}],"source":["!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7039,"status":"ok","timestamp":1635442439014,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"ENUrJ4ywL9ti","outputId":"e10fdaa8-2368-4c1b-b074-6d4c4eaafb3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 36.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.9.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n","Collecting boto3\n","  Downloading boto3-1.19.5-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 56.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 65.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n","Collecting botocore<1.23.0,>=1.22.5\n","  Downloading botocore-1.22.5-py3-none-any.whl (8.1 MB)\n","\u001b[K     |████████████████████████████████| 8.1 MB 35.6 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.5->boto3->pytorch-transformers) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 77.6 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.5->boto3->pytorch-transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 75.2 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.19.5 botocore-1.22.5 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sentencepiece-0.1.96 urllib3-1.25.11\n"]}],"source":["!pip install pytorch-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14597,"status":"ok","timestamp":1635442453608,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"mITSXunyLVR1","outputId":"645fa8f4-76b2-4411-9792-d8e7d106c662"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.12.0\n"]}],"source":["from transformers import __version__\n","print(__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2018,"status":"ok","timestamp":1635442455623,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"tNnZqTbH1Owv","outputId":"e88c529d-cbda-4003-a697-45d62f8ba2a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7112,"status":"ok","timestamp":1635442462733,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"6MiGYWogQK7L","outputId":"8c08fcfb-c1a2-4556-ed40-7668230876bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wandb\n","  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 4.2 MB/s \n","\u001b[?25hCollecting yaspin>=1.0.0\n","  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 69.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n","\u001b[K     |████████████████████████████████| 139 kB 61.0 MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.9 MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=7167d3a425577760297ea2d21848d630410694ee1a144a9e2110d5c7cab6f88b\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=dda6b755de2fb949cdde95aa508900d590752497e0b096c3d383df9bccecb216\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.4.3 shortuuid-1.0.1 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.6 yaspin-2.1.0\n"]}],"source":["!pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"beCCkJ6DQBtf"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2121,"status":"ok","timestamp":1635442464852,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"GweU62UIQIqJ","outputId":"f3906d7e-d49b-4c33-daf8-1e9eafd98176"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import regex\n","from tqdm import tqdm\n","\n","from sklearn import preprocessing, metrics\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n","from torch.utils.data import Dataset, DataLoader\n","\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow as tf\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import RobertaModel, RobertaTokenizer\n","\n","import nltk\n","from nltk.corpus import stopwords \n","from nltk.tokenize import WordPunctTokenizer\n","from string import punctuation\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1635442464852,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"cupnx6q_vNb2","outputId":"e60dc159-3b79-4069-e3d0-2f5d4e623788"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.9.0+cu111\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tayL3He7uvF8"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_9qOO8LiN2p"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"0KgP7rRLQFZO"},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfutfGKBQAlH"},"outputs":[],"source":["f_path = '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/dataset/crowdflower_text_emotion.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgpvlBcMQAnH"},"outputs":[],"source":["df = pd.read_csv(f_path)"]},{"cell_type":"markdown","metadata":{"id":"OuzN46oCQMOc"},"source":["### Data preprocessing and analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1635442468806,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"oGKcv1-GQApG","outputId":"83a20387-3ae2-4a30-d520-359e165a9808"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     tweet_id  ...                                            content\n","0  1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n","1  1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696  ...                Funeral ceremony...gloomy friday...\n","3  1956967789  ...               wants to hang out with friends SOON!\n","4  1956968416  ...  @dannycastillo We want to trade with someone w...\n","\n","[5 rows x 4 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1635442468807,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"T3uI-NOsQArQ","outputId":"087d97f4-7c36-494c-e71c-438b596506cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n"," 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"]}],"source":["# Check emotions labels\n","\n","labels = df.sentiment.unique()\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KG7DyxzPQAtI"},"outputs":[],"source":["# Remove irrelevant columns\n","df = df.drop(columns=['tweet_id', 'author'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1635442468807,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"IA3PHfffNONe","outputId":"e5526a27-5bbd-484e-f437-fd5d2c69aa44"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Check for null cells\n","df['sentiment'].isnull().values.any()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lw6l6A1i7vUk"},"outputs":[],"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","\n","stop = stopwords.words('english')\n","\n","for punct in punctuation:\n","    stop.append(punct)\n","\n","def clean(x, stop_words):\n","    word_tokens = WordPunctTokenizer().tokenize(x.lower())\n","    x = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 2]\n","    return \" \".join(x)\n","\n","df[\"content_cleaned\"] = df.content.apply(lambda x : clean(x, stop)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNrhUEtWNOPz"},"outputs":[],"source":["# Check len of each content\n","df['content_count'] = df['content_cleaned'].str.split(\" \").str.len()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1635442471544,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"BKJ0zVzG_4Di","outputId":"556e745a-fc4b-428f-f62d-2fc309399eae"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","      <th>content_cleaned</th>\n","      <th>content_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>empty</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>tiffanylue know was listenin bad habit earlier...</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>layin bed with headache ughhhh waitin your call</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>wants hang out with friends soon</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>dannycastillo want trade with someone who has ...</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    sentiment  ... content_count\n","0       empty  ...            12\n","1     sadness  ...             8\n","2     sadness  ...             4\n","3  enthusiasm  ...             6\n","4     neutral  ...            12\n","\n","[5 rows x 4 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"RJ9ZRe_4BGGf"},"source":["### Label encoding classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdPWKgwWF_d-"},"outputs":[],"source":["labels_dict = dict(zip(labels, (x for x in range(len(labels)) )))\n","df['sentiment_val'] = df['sentiment'].map(labels_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLvTvEDeAggz"},"outputs":[],"source":["df = df.drop(columns=['sentiment', 'content_count', 'content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1635442471545,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"phXezD-LAqab","outputId":"2d04acdf-16e0-4edf-d42a-71d570c31d9e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content_cleaned</th>\n","      <th>sentiment_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tiffanylue know was listenin bad habit earlier...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>layin bed with headache ughhhh waitin your call</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>funeral ceremony gloomy friday</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>wants hang out with friends soon</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>dannycastillo want trade with someone who has ...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     content_cleaned  sentiment_val\n","0  tiffanylue know was listenin bad habit earlier...              0\n","1    layin bed with headache ughhhh waitin your call              1\n","2                     funeral ceremony gloomy friday              1\n","3                   wants hang out with friends soon              2\n","4  dannycastillo want trade with someone who has ...              3"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"Iz3SZaTNYQQM"},"source":["### Prep data for training/validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KuaiJhtENUQ"},"outputs":[],"source":["class SentimentData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.content_cleaned\n","        self.targets = self.data.sentiment_val\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4xJ5o_xiXM1"},"outputs":[],"source":["# Batch Size\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YULG6eb_KjTz"},"outputs":[],"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 2\n","                }\n","\n","val_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 2\n","                }"]},{"cell_type":"markdown","metadata":{"id":"L-pnKfdrjE1A"},"source":["### RoBERTa models"]},{"cell_type":"markdown","metadata":{"id":"6DKbJsD9hXIC"},"source":["#### Base model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["aada92b9383146caa0f86c857edd72fd","dea1d0e8589749b39111cb12b6abd09d","14075fffcf58428f88adc6478efee818","7ce16e64575d42d5a13b455e1e50efe1","c4d60538beca407ab833c1eebc77a117","9db3a4cc6e2044da92ddd4f3cfa9ce0d","0eac4a4ee66f4c4b877ba251f70b9470","41c2634c34b74ddfa8011c87b5206ae9","3e605b765d8247d1aa8270d123d85a38","e9bbb1aeb14a4bfeac8955131748bade","808454f78a51438c9c19166677e9a196","6dd8e4b523f64be9b6856612413249f3","a9536b46f7914fde92f551889abcd9cb","acb2c55c1f7d43348abd1d6ffed65064","f136dc7e2f3141499d4eac10db60dfdc","8ea61bef2bcc47c785617548aaf27cd2","d5318963220e4558bd3ad5b23b6b57de","3f13d5dd5c77495ab3a0912397a6f661","20a55bf5b5a34338a2ec32dcfcdf905b","5622ec85f9ab4860a1de257ae147d4a0","3157e52442034a3e89cfef738a83ce63","dbf29298f6a44394a0ed386ea03ade21","5a42a6688e514217bb222d71a51ab862","fc7602431d7540f9860f50863774642d","16d1d0ea13144821959f2cdada71d434","02001376d1e842b3bbcfaf4cd02ec946","cf83135206164b5287e07cebbdf11b0f","d4643a06228c4cddbc1233d35fd74f61","f3599a24841b4ca3b9a13f9d434830e7","abd8515ad80143548ec5ebadaba66647","09330c9915a942e7bf37fb3d88bd5bd6","eedad23ffe0a42b2b329b1714b7a5c5f","6eb093bae27f4d4aaa14b21d60bcd921","9076b065c9d4406e81a17321b2e21a35","3a2aa1f28cf34edeadcf40bd0b4c923b","d0a68b15136e48048a81080eafd53630","4cf08bd670d44487bdcb40c818402f87","7840e041995c40f7bee9d43311194acd","8388e0d8abe141209eb4fabe52f1b399","57092d35404c4caeb4de771c76a65daf","cef7f9472369461c953918e5f205b331","c3b2c7498c8d4ca7b8fa044265ec7d1f","0ca9c571c8404b70b6611146d400c9c8","c17ff34ad31243a9a3906d41656ba5ec"]},"executionInfo":{"elapsed":12402,"status":"ok","timestamp":1635442483939,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"KC5X1knahvyw","outputId":"dcd54397-6d53-40d7-f307-16cbaf68e113"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aada92b9383146caa0f86c857edd72fd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dd8e4b523f64be9b6856612413249f3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a42a6688e514217bb222d71a51ab862","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9076b065c9d4406e81a17321b2e21a35","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Initialize tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28ao1VRyjEQw"},"outputs":[],"source":["class RobertaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(RobertaClass, self).__init__()\n","        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 13)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Un7FvN-UiJ43"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yJR_LDAQNjaR"},"source":["### Train/Eval functions"]},{"cell_type":"markdown","metadata":{"id":"6sBbhhzJiENf"},"source":["#### Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkXQL8QDiDwh"},"outputs":[],"source":["LEARNING_RATE = 1e-05\n","MAX_LEN = 256\n","\n","# Multi class loss\n","loss_function = torch.nn.CrossEntropyLoss()\n","\n","# Accuracy function\n","def calcuate_accuracy(preds, targets):\n","    n_correct = (preds==targets).sum().item()\n","    return n_correct"]},{"cell_type":"markdown","metadata":{"id":"hpDp_VFKjI3_"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWl0UERQi_3C"},"outputs":[],"source":["# For RoBERTa base\n","def train(model, epoch, training_loader):\n","\n","  tr_loss = 0\n","  n_correct = 0\n","  nb_tr_steps = 0\n","  nb_tr_examples = 0\n","\n","  # Tell wandb to watch this model train\n","  wandb.watch(model)\n","\n","  model.train()\n","\n","  for _, data in tqdm(enumerate(training_loader, 0)):\n","\n","    ids = data['ids'].to(device, dtype = torch.long)\n","    mask = data['mask'].to(device, dtype = torch.long)\n","    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","    targets = data['targets'].to(device, dtype = torch.long)\n","\n","    outputs = model(ids, mask, token_type_ids)\n","    loss = loss_function(outputs, targets)\n","    tr_loss += loss.item()\n","    big_val, big_idx = torch.max(outputs.data, dim=1)\n","    n_correct += calcuate_accuracy(big_idx, targets)\n","\n","    nb_tr_steps += 1\n","    nb_tr_examples+=targets.size(0)\n","    \n","    if _%5000==0:\n","        loss_step = tr_loss/nb_tr_steps\n","        accu_step = (n_correct*100)/nb_tr_examples \n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Optimizer \n","    optimizer.step()\n","\n","  print('\\n')\n","  epoch_loss = tr_loss/nb_tr_steps\n","  epoch_accu = (n_correct*100)/nb_tr_examples\n","  print(f\"Training Loss Epoch {epoch+1}: {epoch_loss}\")\n","  print(f\"Training Accuracy Epoch {epoch+1}: {epoch_accu} \\n\")\n","  \n","  return epoch_accu, epoch_loss"]},{"cell_type":"markdown","metadata":{"id":"5Gpk3QhujbD2"},"source":["#### Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGGf2WT9jKyS"},"outputs":[],"source":["def valid(model, testing_loader):\n","\n","    n_correct = 0; \n","    n_wrong = 0; \n","    total = 0; \n","    tr_loss=0; \n","    nb_tr_steps=0; \n","    nb_tr_examples=0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for _, data in tqdm(enumerate(testing_loader, 0)):\n","\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype = torch.long)\n","            outputs = model(ids, mask, token_type_ids).squeeze()\n","            loss = loss_function(outputs, targets)\n","            tr_loss += loss.item()\n","            big_val, big_idx = torch.max(outputs.data, dim=1)\n","            n_correct += calcuate_accuracy(big_idx, targets)\n","\n","            nb_tr_steps += 1\n","            nb_tr_examples+=targets.size(0)\n","            \n","            if _%5000==0:\n","                loss_step = tr_loss/nb_tr_steps\n","                accu_step = (n_correct*100)/nb_tr_examples\n","        # Reduce LR on val loss plateau\n","        scheduler.step(loss)\n","\n","    print('\\n')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Validation Loss Epoch : {epoch_loss}\")\n","    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n","    \n","    return epoch_accu, epoch_loss\n"]},{"cell_type":"markdown","metadata":{"id":"g4wUnrjUHsSq"},"source":["### K-Fold (RoBERTa base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYqaYA4fjK0R"},"outputs":[],"source":["# Set K-fold\n","kf = KFold(n_splits=3, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":7605,"status":"ok","timestamp":1635442496941,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"MqVdGDcavQpa","outputId":"fcb9b9a8-2dcc-4bac-b6e2-8d35df82f035"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# # Setup wandb for logging\n","# import wandb\n","\n","# wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fe9c901ab8bd47d19568d3dc5c17a945","733505ab1736449db0ef0dc60ef42935","3aab7236dd0e4ce9b19c57731ec025ea","8a7ffb7c027641728a487d4d7c08af09","0974b43eee3b482f961643b5efae5340","01e3f53b5c4a4a62b33a059df03b42fd","dde9d5da335d458da1cdce3847ef51ec","9641e1cf71d6462eb08242e718275225","db8b26b93bc74072b37c4d215347d0af","8aa298f87de94f38a8f9b4a78cdd5fbf","06eb892cc2c542c189e78798ee861f0d","d727120181564407b7976fce8c47e94d","96e9dbfc7ca8474a8f9484de2998cdc0","63eba31a6d0c4db7aa578f04cb5b7392","39c54c282ebd4daba63cd9c43690c200","ecaab0430f88411ca62bedddbe3f00dd","bcec90b34c1a4dff8600bcbbf52640c7","381818efe9a04fa0b77cdadb8c01aa18","55f0040cb8dc4ea3bf5110b7bc375d33","04347eae0fc34c728a8284cb7f5fe907","2f6ff437c0ad43c8bad21134f3bd2917","9fa77ad29f2642729e06e9bb529e1df4","97048d1634194f478fa3df5bc77eb81f","9d5a7acaa33344cab43b4998cfa7c2e9","d51f1d65b9ec4682ab4e9bd81057f286","2c14571e89cc41a2acff2f5e79463b88","0ba10b210b0a4700a0f0c1dd4914e01a"]},"executionInfo":{"elapsed":39430652,"status":"ok","timestamp":1635481955232,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"KEfTFkBEGv6J","outputId":"9fa0f476-8ec4-4189-e619-3bc3f6bd9e80"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe9c901ab8bd47d19568d3dc5c17a945","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwasabee\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/2fwhtlx7\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 1...\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:23,  4.49it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 1.9214037464418738\n","Training Accuracy Epoch 1: 34.57961449036226 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7907606852433606\n","Validation Accuracy Epoch: 41.45792710364482\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:24,  4.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 1.7785669701787334\n","Training Accuracy Epoch 2: 39.89724743118578 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7386959775093436\n","Validation Accuracy Epoch: 42.17039148042598\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:26,  4.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 1.678431823572572\n","Training Accuracy Epoch 3: 43.227330683267084 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.6164623301545518\n","Validation Accuracy Epoch: 47.60761961901905\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:27,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 1.5680188124202723\n","Training Accuracy Epoch 4: 46.97367434185855 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.5401196636407049\n","Validation Accuracy Epoch: 50.63746812659367\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:28,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 1.442215818493897\n","Training Accuracy Epoch 5: 51.95754893872347 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.4807116295237275\n","Validation Accuracy Epoch: 53.217339133043346\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:30,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 1.3029814386571605\n","Training Accuracy Epoch 6: 56.56641416035401 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.3544905647483498\n","Validation Accuracy Epoch: 58.47457627118644\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:31,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.1560176061159062\n","Training Accuracy Epoch 7: 62.09780244506113 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2921248520822244\n","Validation Accuracy Epoch: 62.23938803059847\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:33,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 1.0068044939134817\n","Training Accuracy Epoch 8: 67.01417535438387 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2409791355281086\n","Validation Accuracy Epoch: 64.81925903704814\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:35,  4.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 0.8647539897027957\n","Training Accuracy Epoch 9: 71.67929198229956 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1843843393532831\n","Validation Accuracy Epoch: 69.11654417279136\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:36,  4.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 0.7282741280997772\n","Training Accuracy Epoch 10: 76.35190879771994 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1699027583588542\n","Validation Accuracy Epoch: 70.95395230238488\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:38,  4.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 0.5015001143968536\n","Training Accuracy Epoch 11: 84.05835145878648 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1755682383756134\n","Validation Accuracy Epoch: 72.19139043047848\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:39,  4.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 0.44934977295662676\n","Training Accuracy Epoch 12: 86.00840021000525 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1808101241557842\n","Validation Accuracy Epoch: 72.72386380680966\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:41,  4.38it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 0.4192892447170976\n","Training Accuracy Epoch 13: 86.83342083552088 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1928440404661451\n","Validation Accuracy Epoch: 73.24883755812209\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:42,  4.37it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 0.38932971269267724\n","Training Accuracy Epoch 14: 87.86094652366309 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2041584624136208\n","Validation Accuracy Epoch: 73.21133943302834\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:44,  4.36it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 0.36201475351187384\n","Training Accuracy Epoch 15: 88.86972174304357 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2060809790491431\n","Validation Accuracy Epoch: 73.40632968351582\n","Epoch 15 done!\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/html":["Finishing last run (ID:2fwhtlx7) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 385... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d727120181564407b7976fce8c47e94d","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.25MB of 0.25MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Acc</td><td>▁▂▂▃▃▄▅▅▆▆▇████</td></tr><tr><td>Train_Loss</td><td>█▇▇▆▆▅▅▄▃▃▂▁▁▁▁</td></tr><tr><td>Val_Acc</td><td>▁▁▂▃▄▅▆▆▇▇█████</td></tr><tr><td>Val_Loss</td><td>█▇▆▅▅▃▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>Train_Acc</td><td>88.86972</td></tr><tr><td>Train_Loss</td><td>0.36201</td></tr><tr><td>Val_Acc</td><td>73.40633</td></tr><tr><td>Val_Loss</td><td>1.20608</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">3-Fold</strong>: <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/2fwhtlx7\" target=\"_blank\">https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/2fwhtlx7</a><br/>\n","Find logs at: <code>./wandb/run-20211028_173550-2fwhtlx7/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:2fwhtlx7). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/32aabg8w\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 2...\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:23,  4.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 1.9148643163770849\n","Training Accuracy Epoch 1: 34.93081336483294 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7945616074357837\n","Validation Accuracy Epoch: 41.08602715067877\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:25,  4.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 1.7731627195662343\n","Training Accuracy Epoch 2: 40.14699816252297 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.706192557405553\n","Validation Accuracy Epoch: 44.12360309007725\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:26,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 1.6751615675216531\n","Training Accuracy Epoch 3: 43.341958225522184 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.633653112099996\n","Validation Accuracy Epoch: 46.726168154203854\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:28,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 1.566849661627857\n","Training Accuracy Epoch 4: 47.519406007424905 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.538495494637149\n","Validation Accuracy Epoch: 50.30375759393985\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:29,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 1.4393841934887035\n","Training Accuracy Epoch 5: 52.40184497693779 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.472293949030657\n","Validation Accuracy Epoch: 53.71634290857271\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:31,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 1.3064605572227954\n","Training Accuracy Epoch 6: 56.973037837027036 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.368622774482846\n","Validation Accuracy Epoch: 57.751443786094654\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:32,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.1644028130187984\n","Training Accuracy Epoch 7: 61.79922750965613 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2943380354011425\n","Validation Accuracy Epoch: 61.749043726093156\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:33,  4.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 1.015832026078448\n","Training Accuracy Epoch 8: 67.34915813552331 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2529179348460056\n","Validation Accuracy Epoch: 64.71911797794945\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:35,  4.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 0.8852343726132219\n","Training Accuracy Epoch 9: 71.56035549555631 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1891328257424214\n","Validation Accuracy Epoch: 67.64419110477762\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:37,  4.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 0.7620517698155149\n","Training Accuracy Epoch 10: 75.52405594930063 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.15718855901471\n","Validation Accuracy Epoch: 69.55673891847296\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:38,  4.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 0.6467961436724173\n","Training Accuracy Epoch 11: 79.25900926238423 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1553298091816468\n","Validation Accuracy Epoch: 71.63429085727144\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:40,  4.38it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 0.5456381594093463\n","Training Accuracy Epoch 12: 82.48021899726254 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1670497322558964\n","Validation Accuracy Epoch: 73.21683042076052\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:41,  4.38it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 0.46441828506542276\n","Training Accuracy Epoch 13: 85.25518431019613 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2335290874538347\n","Validation Accuracy Epoch: 72.24930623265581\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:43,  4.37it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 0.40745479518399047\n","Training Accuracy Epoch 14: 86.96516293546331 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.260280635915462\n","Validation Accuracy Epoch: 73.70434260856521\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:45,  4.36it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 0.25754428551790437\n","Training Accuracy Epoch 15: 91.76135298308772 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2534313240414234\n","Validation Accuracy Epoch: 74.9268731718293\n","Epoch 15 done!\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/html":["Finishing last run (ID:32aabg8w) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 1540... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04347eae0fc34c728a8284cb7f5fe907","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train_Acc</td><td>▁▂▂▃▃▄▄▅▆▆▆▇▇▇█</td></tr><tr><td>Train_Loss</td><td>█▇▇▇▆▅▅▄▄▃▃▂▂▂▁</td></tr><tr><td>Val_Acc</td><td>▁▂▂▃▄▄▅▆▆▇▇█▇██</td></tr><tr><td>Val_Loss</td><td>█▇▆▅▄▃▃▂▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>Train_Acc</td><td>91.76135</td></tr><tr><td>Train_Loss</td><td>0.25754</td></tr><tr><td>Val_Acc</td><td>74.92687</td></tr><tr><td>Val_Loss</td><td>1.25343</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">3-Fold</strong>: <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/32aabg8w\" target=\"_blank\">https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/32aabg8w</a><br/>\n","Find logs at: <code>./wandb/run-20211028_211420-32aabg8w/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:32aabg8w). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa/runs/cih7n4c1\" target=\"_blank\">3-Fold</a></strong> to <a href=\"https://wandb.ai/wasabee/Text_Emotion_Recognition_RoBERTa\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training fold 3...\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:24,  4.48it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 1: 1.9178336758812389\n","Training Accuracy Epoch 1: 34.68706641166985 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.8114483825232215\n","Validation Accuracy Epoch: 39.615990399759994\n","Epoch 1 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:25,  4.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 2: 1.769017270101926\n","Training Accuracy Epoch 2: 40.323245959425506 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.7092555076616855\n","Validation Accuracy Epoch: 44.303607590189756\n","Epoch 2 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:27,  4.46it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 3: 1.6663275249324734\n","Training Accuracy Epoch 3: 43.889451381857725 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [01:59, 13.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.6162750220398883\n","Validation Accuracy Epoch: 47.73869346733668\n","Epoch 3 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:28,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 4: 1.5542103603401796\n","Training Accuracy Epoch 4: 48.04814939813252 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.5170351754746612\n","Validation Accuracy Epoch: 51.1812795319883\n","Epoch 4 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:30,  4.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 5: 1.4262902310277863\n","Training Accuracy Epoch 5: 52.76559043011962 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.4527874210314187\n","Validation Accuracy Epoch: 53.866346658666465\n","Epoch 5 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:31,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 6: 1.2824907120413314\n","Training Accuracy Epoch 6: 57.970525368432895 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.3203055095408016\n","Validation Accuracy Epoch: 59.31898297457436\n","Epoch 6 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:33,  4.43it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 7: 1.1338769439127392\n","Training Accuracy Epoch 7: 63.17171035362058 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2793248842845224\n","Validation Accuracy Epoch: 62.36405910147754\n","Epoch 7 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:34,  4.42it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 8: 0.9836407521874803\n","Training Accuracy Epoch 8: 68.18539768252897 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1988882769723817\n","Validation Accuracy Epoch: 66.57166429160729\n","Epoch 8 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:36,  4.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 9: 0.8384903229875842\n","Training Accuracy Epoch 9: 73.03408707391158 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1595047058266084\n","Validation Accuracy Epoch: 68.95672391809795\n","Epoch 9 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:38,  4.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 10: 0.715878955797523\n","Training Accuracy Epoch 10: 76.84403944950688 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1567491415684712\n","Validation Accuracy Epoch: 71.33428335708393\n","Epoch 10 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:39,  4.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 11: 0.6012935596515425\n","Training Accuracy Epoch 11: 80.63524205947425 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.1312865302409143\n","Validation Accuracy Epoch: 72.69931748293708\n","Epoch 11 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:41,  4.38it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 12: 0.5106336787207825\n","Training Accuracy Epoch 12: 83.60520493493831 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.173148052293157\n","Validation Accuracy Epoch: 73.21683042076052\n","Epoch 12 done!\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:42,  4.37it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 13: 0.42340768043378657\n","Training Accuracy Epoch 13: 86.27892151348108 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.196308304409898\n","Validation Accuracy Epoch: 74.96437410935273\n","Epoch 13 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:44,  4.36it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 14: 0.37082900864423557\n","Training Accuracy Epoch 14: 88.15014812314845 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.2084762191963643\n","Validation Accuracy Epoch: 75.67689192229805\n","Epoch 14 done!\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","3334it [12:46,  4.35it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Training Loss Epoch 15: 0.32960516793612027\n","Training Accuracy Epoch 15: 89.19638504518693 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\n","0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","1667it [02:00, 13.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Validation Loss Epoch : 1.261165333744172\n","Validation Accuracy Epoch: 75.27938198454962\n","Epoch 15 done!\n"]}],"source":["EPOCHS=15\n","f = 1\n","df = df.reset_index(drop=True)\n","\n","for train_index, val_index in kf.split(df):\n","\n","  # Init model\n","  model = RobertaClass()\n","  model.to(device)\n","\n","  # Optimizer\n","  optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n","  # Reduce LR on plateau\n","  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n","\n","  # Init wandb for logging\n","  # wandb.init(project=\"Text_Emotion_Recognition_RoBERTa\", name=f\"3-Fold\")\n","\n","  train2 = df.iloc[train_index]\n","  val2 = df.iloc[val_index]\n","\n","  train2 = train2.reset_index(drop=True)\n","  val2 = df.drop(train2.index).reset_index(drop=True)\n","\n","  training_set = SentimentData(train2, tokenizer, MAX_LEN)\n","  val_set = SentimentData(val2, tokenizer, MAX_LEN)\n","\n","  training_loader = DataLoader(training_set, **train_params)\n","  val_loader = DataLoader(val_set, **val_params)\n","\n","  print(f\"Training fold {f}...\")\n","\n","  for epoch in range(0, EPOCHS):\n","    \n","    # Train\n","    train_acc, train_loss = train(model, epoch, training_loader)\n","    # Val\n","    val_acc, val_loss = valid(model, val_loader)\n","    print(f\"Epoch {epoch+1} done!\")\n","    # Log metrics\n","    # wandb.log({\"Train_Loss\": train_loss, \"Val_Loss\": val_loss, \"Train_Acc\": train_acc, \"Val_Acc\": val_acc, \"Epoch\": epoch})   \n","\n","  f+=1\n","  # wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"jUfUWS5-It4a"},"source":["### Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7135,"status":"ok","timestamp":1635481962365,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"X9PXHnI-IowG","outputId":"efd14780-958d-4a03-9414-804bae758c3a"},"outputs":[{"data":{"text/plain":["('/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/roberta_base/vocab.json',\n"," '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/roberta_base/merges.txt')"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# Base model\n","rob_base = model\n","\n","torch.save(rob_base, '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/roberta_base/roberta_base.bin')\n","tokenizer.save_vocabulary('/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/roberta_base/')"]},{"cell_type":"markdown","metadata":{"id":"lW0sM4IRQX7a"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"LEmMcwr7UICq"},"source":["#### Load model, tokenizer, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaM7DP90W5kC"},"outputs":[],"source":["!cp '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/roberta_sent.bin' -d '/content/roberta_sent.bin'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzQhFQcvXshC"},"outputs":[],"source":["!cp '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/vocab.json' -d '/content/vocab.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVfm8mL3Xsmv"},"outputs":[],"source":["!cp '/content/drive/MyDrive/3.1/CZ4042 Neural Networks & Deep Learning/Project/emotion/models/merges.txt' -d '/content/merges.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBp5LWBbZFOr"},"outputs":[],"source":["class RobertaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(RobertaClass, self).__init__()\n","        self.l1 = RobertaModel.from_pretrained(\"/content/roberta_sent.bin\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 13)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57V7Rtl0XhNT"},"outputs":[],"source":["saved_model2 = torch.load('/content/roberta_sent.bin')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29300,"status":"ok","timestamp":1634806099775,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"mes6nLXTXnp1","outputId":"e6e40695-065e-4251-9b6f-64e4477f0819"},"outputs":[{"name":"stderr","output_type":"stream","text":["file /content/config.json not found\n"]}],"source":["tokenizer2 = RobertaTokenizer.from_pretrained(\"/content/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8x6U8DKVuUj"},"outputs":[],"source":["labels_dict = {'anger': 12,\n"," 'boredom': 10,\n"," 'empty': 0,\n"," 'enthusiasm': 2,\n"," 'fun': 7,\n"," 'happiness': 9,\n"," 'hate': 8,\n"," 'love': 6,\n"," 'neutral': 3,\n"," 'relief': 11,\n"," 'sadness': 1,\n"," 'surprise': 5,\n"," 'worry': 4}"]},{"cell_type":"markdown","metadata":{"id":"UbJwWsZ7UKFL"},"source":["#### Function to get encode user input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnFC7cIeOPcF"},"outputs":[],"source":["def get_inputs(text, tokenizer):\n","\n","  inps = tokenizer.encode_plus(\n","              list(text),\n","              None,\n","              add_special_tokens=True,\n","              max_length=256,\n","              pad_to_max_length=True,\n","              return_token_type_ids=True,\n","              return_tensors='pt'\n","          )\n","  \n","  inp_tok = inps['input_ids'].to('cuda', dtype = torch.long)\n","  ids = inps['attention_mask'].to('cuda', dtype = torch.long)\n","  segments = inps['token_type_ids'].to('cuda', dtype = torch.long)\n","\n","  return inp_tok, ids, segments\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAw_lBiRV-08"},"outputs":[],"source":["def cleaner(user_text):\n","\n","  wordnet_lemmatizer = WordNetLemmatizer()\n","\n","  stop = stopwords.words('english')\n","\n","  for punct in punctuation:\n","      stop.append(punct)\n","\n","  def clean(x, stop_words):\n","      word_tokens = WordPunctTokenizer().tokenize(x.lower())\n","      x = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 2]\n","      return \" \".join(x)\n","\n","  cleaned_user = clean(user_text, stop) \n","  \n","  return cleaned_user"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2KsocB1UQBz"},"outputs":[],"source":["def get_sentiment(user_input):\n","\n","  # Define empty list to store emotions\n","  sentiment = []\n","\n","  user_input = cleaner(user_input)\n","\n","  # Pass user input to encode text\n","  inp_tok, ids, segments = get_inputs(user_input, tokenizer2)\n","\n","  # Get predictions\n","  preds = saved_model2(inp_tok, ids, segments)\n","\n","  # Get max pred\n","  # big_val, big_idx = torch.max(preds.data, dim=1)\n","  big_val, big_idx = torch.topk(preds.data, k=3, dim=1)\n","\n","  for x in range(len(big_idx[0])):\n","    sentiment.append(list(labels_dict.keys())[list(labels_dict.values()).index(big_idx[0][x].item())])\n","\n","  print(\"Predicted Sentiment:\", sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_KQAn53SMV5"},"outputs":[],"source":["# write to get user input here\n","# user_input = input(\"Enter some text: \")\n","user_input = 'missed the bus.... shit!'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1634806466286,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"J47UT-t9SIfi","outputId":"a80d2f8a-3d69-4388-f688-85855528c949"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Sentiment: ['sadness', 'neutral', 'worry']\n"]}],"source":["get_sentiment(user_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1634794950040,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"okv4zPwmQXSC","outputId":"fe93b98d-57da-490c-b26e-cf9b73c3da9d"},"outputs":[{"data":{"text/plain":["{'anger': 12,\n"," 'boredom': 10,\n"," 'empty': 0,\n"," 'enthusiasm': 2,\n"," 'fun': 7,\n"," 'happiness': 9,\n"," 'hate': 8,\n"," 'love': 6,\n"," 'neutral': 3,\n"," 'relief': 11,\n"," 'sadness': 1,\n"," 'surprise': 5,\n"," 'worry': 4}"]},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":["labels_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":718,"status":"ok","timestamp":1634794965966,"user":{"displayName":"Jun Xian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdSvjzNJuPRvBm68e_4yYL4N9PUSuGs-FMUyCBog=s64","userId":"17208504523579366933"},"user_tz":-480},"id":"jkMtSETJR3pK","outputId":"654a81c4-b675-4d9d-8d47-9623768e7478"},"outputs":[{"name":"stdout","output_type":"stream","text":["neutral\n","missed the bus\n"]}],"source":["from random import randint\n","\n","r_int = randint(0,100)\n","\n","print(df.sentiment[r_int])\n","print(df.content_cleaned[r_int])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OW9pCCPaQJa6","beCCkJ6DQBtf","0KgP7rRLQFZO","RJ9ZRe_4BGGf","6sBbhhzJiENf","hpDp_VFKjI3_","5Gpk3QhujbD2","jUfUWS5-It4a","lW0sM4IRQX7a","LEmMcwr7UICq","UbJwWsZ7UKFL"],"name":"RoBERTa_base.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01e3f53b5c4a4a62b33a059df03b42fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02001376d1e842b3bbcfaf4cd02ec946":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09330c9915a942e7bf37fb3d88bd5bd6","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abd8515ad80143548ec5ebadaba66647","value":1355863}},"04347eae0fc34c728a8284cb7f5fe907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_9fa77ad29f2642729e06e9bb529e1df4","IPY_MODEL_97048d1634194f478fa3df5bc77eb81f"],"layout":"IPY_MODEL_2f6ff437c0ad43c8bad21134f3bd2917"}},"06eb892cc2c542c189e78798ee861f0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09330c9915a942e7bf37fb3d88bd5bd6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0974b43eee3b482f961643b5efae5340":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06eb892cc2c542c189e78798ee861f0d","placeholder":"​","style":"IPY_MODEL_8aa298f87de94f38a8f9b4a78cdd5fbf","value":" 478M/478M [00:10&lt;00:00, 52.8MB/s]"}},"0ba10b210b0a4700a0f0c1dd4914e01a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca9c571c8404b70b6611146d400c9c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0eac4a4ee66f4c4b877ba251f70b9470":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14075fffcf58428f88adc6478efee818":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eac4a4ee66f4c4b877ba251f70b9470","placeholder":"​","style":"IPY_MODEL_9db3a4cc6e2044da92ddd4f3cfa9ce0d","value":"Downloading: 100%"}},"16d1d0ea13144821959f2cdada71d434":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3599a24841b4ca3b9a13f9d434830e7","placeholder":"​","style":"IPY_MODEL_d4643a06228c4cddbc1233d35fd74f61","value":"Downloading: 100%"}},"20a55bf5b5a34338a2ec32dcfcdf905b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c14571e89cc41a2acff2f5e79463b88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f6ff437c0ad43c8bad21134f3bd2917":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3157e52442034a3e89cfef738a83ce63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"381818efe9a04fa0b77cdadb8c01aa18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39c54c282ebd4daba63cd9c43690c200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_55f0040cb8dc4ea3bf5110b7bc375d33","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_381818efe9a04fa0b77cdadb8c01aa18","value":1}},"3a2aa1f28cf34edeadcf40bd0b4c923b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aab7236dd0e4ce9b19c57731ec025ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dde9d5da335d458da1cdce3847ef51ec","placeholder":"​","style":"IPY_MODEL_01e3f53b5c4a4a62b33a059df03b42fd","value":"Downloading: 100%"}},"3e605b765d8247d1aa8270d123d85a38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f13d5dd5c77495ab3a0912397a6f661":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41c2634c34b74ddfa8011c87b5206ae9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4cf08bd670d44487bdcb40c818402f87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3b2c7498c8d4ca7b8fa044265ec7d1f","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cef7f9472369461c953918e5f205b331","value":481}},"55f0040cb8dc4ea3bf5110b7bc375d33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5622ec85f9ab4860a1de257ae147d4a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57092d35404c4caeb4de771c76a65daf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a42a6688e514217bb222d71a51ab862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16d1d0ea13144821959f2cdada71d434","IPY_MODEL_02001376d1e842b3bbcfaf4cd02ec946","IPY_MODEL_cf83135206164b5287e07cebbdf11b0f"],"layout":"IPY_MODEL_fc7602431d7540f9860f50863774642d"}},"63eba31a6d0c4db7aa578f04cb5b7392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcec90b34c1a4dff8600bcbbf52640c7","placeholder":"​","style":"IPY_MODEL_ecaab0430f88411ca62bedddbe3f00dd","value":" 5.75MB of 5.75MB uploaded (0.00MB deduped)\r"}},"6dd8e4b523f64be9b6856612413249f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acb2c55c1f7d43348abd1d6ffed65064","IPY_MODEL_f136dc7e2f3141499d4eac10db60dfdc","IPY_MODEL_8ea61bef2bcc47c785617548aaf27cd2"],"layout":"IPY_MODEL_a9536b46f7914fde92f551889abcd9cb"}},"6eb093bae27f4d4aaa14b21d60bcd921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"733505ab1736449db0ef0dc60ef42935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7840e041995c40f7bee9d43311194acd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c17ff34ad31243a9a3906d41656ba5ec","placeholder":"​","style":"IPY_MODEL_0ca9c571c8404b70b6611146d400c9c8","value":" 481/481 [00:00&lt;00:00, 12.2kB/s]"}},"7ce16e64575d42d5a13b455e1e50efe1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e605b765d8247d1aa8270d123d85a38","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41c2634c34b74ddfa8011c87b5206ae9","value":898823}},"808454f78a51438c9c19166677e9a196":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8388e0d8abe141209eb4fabe52f1b399":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a7ffb7c027641728a487d4d7c08af09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db8b26b93bc74072b37c4d215347d0af","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9641e1cf71d6462eb08242e718275225","value":501200538}},"8aa298f87de94f38a8f9b4a78cdd5fbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ea61bef2bcc47c785617548aaf27cd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbf29298f6a44394a0ed386ea03ade21","placeholder":"​","style":"IPY_MODEL_3157e52442034a3e89cfef738a83ce63","value":" 446k/446k [00:00&lt;00:00, 633kB/s]"}},"9076b065c9d4406e81a17321b2e21a35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0a68b15136e48048a81080eafd53630","IPY_MODEL_4cf08bd670d44487bdcb40c818402f87","IPY_MODEL_7840e041995c40f7bee9d43311194acd"],"layout":"IPY_MODEL_3a2aa1f28cf34edeadcf40bd0b4c923b"}},"9641e1cf71d6462eb08242e718275225":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96e9dbfc7ca8474a8f9484de2998cdc0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97048d1634194f478fa3df5bc77eb81f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ba10b210b0a4700a0f0c1dd4914e01a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c14571e89cc41a2acff2f5e79463b88","value":1}},"9d5a7acaa33344cab43b4998cfa7c2e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9db3a4cc6e2044da92ddd4f3cfa9ce0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fa77ad29f2642729e06e9bb529e1df4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d51f1d65b9ec4682ab4e9bd81057f286","placeholder":"​","style":"IPY_MODEL_9d5a7acaa33344cab43b4998cfa7c2e9","value":" 5.52MB of 5.52MB uploaded (0.00MB deduped)\r"}},"a9536b46f7914fde92f551889abcd9cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aada92b9383146caa0f86c857edd72fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14075fffcf58428f88adc6478efee818","IPY_MODEL_7ce16e64575d42d5a13b455e1e50efe1","IPY_MODEL_c4d60538beca407ab833c1eebc77a117"],"layout":"IPY_MODEL_dea1d0e8589749b39111cb12b6abd09d"}},"abd8515ad80143548ec5ebadaba66647":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acb2c55c1f7d43348abd1d6ffed65064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f13d5dd5c77495ab3a0912397a6f661","placeholder":"​","style":"IPY_MODEL_d5318963220e4558bd3ad5b23b6b57de","value":"Downloading: 100%"}},"bcec90b34c1a4dff8600bcbbf52640c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c17ff34ad31243a9a3906d41656ba5ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3b2c7498c8d4ca7b8fa044265ec7d1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4d60538beca407ab833c1eebc77a117":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_808454f78a51438c9c19166677e9a196","placeholder":"​","style":"IPY_MODEL_e9bbb1aeb14a4bfeac8955131748bade","value":" 878k/878k [00:00&lt;00:00, 990kB/s]"}},"cef7f9472369461c953918e5f205b331":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf83135206164b5287e07cebbdf11b0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eb093bae27f4d4aaa14b21d60bcd921","placeholder":"​","style":"IPY_MODEL_eedad23ffe0a42b2b329b1714b7a5c5f","value":" 1.29M/1.29M [00:01&lt;00:00, 1.31MB/s]"}},"d0a68b15136e48048a81080eafd53630":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57092d35404c4caeb4de771c76a65daf","placeholder":"​","style":"IPY_MODEL_8388e0d8abe141209eb4fabe52f1b399","value":"Downloading: 100%"}},"d4643a06228c4cddbc1233d35fd74f61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d51f1d65b9ec4682ab4e9bd81057f286":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5318963220e4558bd3ad5b23b6b57de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d727120181564407b7976fce8c47e94d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_63eba31a6d0c4db7aa578f04cb5b7392","IPY_MODEL_39c54c282ebd4daba63cd9c43690c200"],"layout":"IPY_MODEL_96e9dbfc7ca8474a8f9484de2998cdc0"}},"db8b26b93bc74072b37c4d215347d0af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbf29298f6a44394a0ed386ea03ade21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dde9d5da335d458da1cdce3847ef51ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea1d0e8589749b39111cb12b6abd09d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9bbb1aeb14a4bfeac8955131748bade":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecaab0430f88411ca62bedddbe3f00dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eedad23ffe0a42b2b329b1714b7a5c5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f136dc7e2f3141499d4eac10db60dfdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5622ec85f9ab4860a1de257ae147d4a0","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20a55bf5b5a34338a2ec32dcfcdf905b","value":456318}},"f3599a24841b4ca3b9a13f9d434830e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc7602431d7540f9860f50863774642d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe9c901ab8bd47d19568d3dc5c17a945":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3aab7236dd0e4ce9b19c57731ec025ea","IPY_MODEL_8a7ffb7c027641728a487d4d7c08af09","IPY_MODEL_0974b43eee3b482f961643b5efae5340"],"layout":"IPY_MODEL_733505ab1736449db0ef0dc60ef42935"}}}}},"nbformat":4,"nbformat_minor":0}
